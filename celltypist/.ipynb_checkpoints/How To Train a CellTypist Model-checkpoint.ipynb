{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e53ad5-a4f9-40af-93f1-0e5e3b10dbd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to train a model with CellTypist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1f262a-e33f-4336-a1b1-8ad8aed58218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import itertools\n",
    "from anndata import AnnData\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "from sklearn import __version__ as skv\n",
    "\n",
    "import cytopus as cp\n",
    "\n",
    "from scipy.sparse import spmatrix\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#import train\n",
    "#import annotate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde301a-6d1e-49d9-bac9-e3eb68320e05",
   "metadata": {},
   "source": [
    "There are a few key components to the Vanilla CellTypist model training: \n",
    "1. It is a Logistic Regression Classifier that uses Standard Gradient Dissent.\n",
    "2. The classifer uses L2 regression \n",
    "3. The model is trained once on all the genes that are provided. Then it performs feature selection where the top 300 genes are selected for every cell type, based on the coefficents from the classifer. The model is then retrained on the union of the selected genes, which is then the final model. \n",
    "4. It uses mini-batch training within each of the epochs\n",
    "\n",
    "For a visual representation of this workflow, see figure 1c from the Conde et al. 2022 paper. \n",
    "\n",
    "In order to train a CellTypist model we need a dataset that has a couple necessary features. Most importantly, this dataset needs to have cell type annotations for each cell that the classifier will use when training the dataset. Additionally, CellTypist works with scRNAseq data so you will need a cellxgene matrix with the raw (or normalized) counts from scRNA sequencing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbaa96-8559-41c7-99af-d5c208849943",
   "metadata": {},
   "source": [
    "## SAIL train function vs the original CellTypist train function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcc489-b8c1-4fa1-bf0a-4acd1b6ba74a",
   "metadata": {},
   "source": [
    "We added some features to the original CellTypist to add some desired functionality:\n",
    "1. The primary addition is that the new train function also returns the list of genes that were chosen during feature selection (if feature selection doesn't happen, an empty dataframe is returned). \n",
    "2. It has the option to integrate Cytopus genes into the feature selection step. Cytopus is a Knowledge base of immune cell types and some of the commonly associated genes for each cell type. By ensuring that Cytopus genes are included in the feature selection, we are incorperating some prior knowledge into our model. \n",
    "2. The new train function also has a second output: a pandas DataFrame that shows which 300 genes were chosen for each cell type during feature selection. \n",
    "3. When training the classifier, we can now choose which sort of regularization to use (eg L1 instead of L2). Additionally, we can use one kind of normalization for the pre-feature selection training and then switch to a different kind for post-feature section training (either L2 -> L1 or L1 -> L2).\n",
    "4. There is now the option to input raw data in AnnData form and then it will be normalized to median library size.\n",
    "\n",
    "\n",
    "In terms of the actual mathematics behind the model, it is pretty much the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c49bd-bdc1-4d21-9ea4-6995fe5904e8",
   "metadata": {},
   "source": [
    "## Our Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d67bac-a726-406a-a465-3d65c75ca574",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199da4c-ab12-4ac6-985f-4636dceda8b9",
   "metadata": {},
   "source": [
    "The easiest type of data to work with when building a CellTypist model is AnnData saved as an h5ad file. The cell type annotations need to be saved in a column in .obs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c9837-1cb7-4408-9f01-1855339b7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "adata = ad.read('')\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36ad7b-856f-42d2-8ec4-58d79893a035",
   "metadata": {},
   "source": [
    "In terms of normalization, we recommend normalizing to median library size of your data (https://www.nature.com/articles/s41592-023-01814-1) or to 10,000 counts per cell and then log normalize. CellTypist expects you to normalize to 10,000 counts per cell (and also log normalize), so it will throw an error if the `check_expression` argument is set to False and it isn't normalized to the expected value. The `check_expression` argument defaults to False. Alternaitvately, you can also imput the raw data and set the `normalize` argument to True (default False). This will normlaize your data to median library size and then log normalize so you do not need to worry about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260808b9-da61-4ce2-a931-7420c1d445e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to check the normalization of your data:\n",
    "np.expm1(adata.X[0]).sum() #the number this produces should be very close to the value you normalized to (just looking at the first cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15567389-913e-48eb-b38d-ba6210f5d7bf",
   "metadata": {},
   "source": [
    "One more thing we recommend you check before moving onto model training is the names of the columns, which should be the gene names. Please ensure that your gene IDs are the actual gene names and not the Ensemble ID's or other similar ID forms. This will produce a model that is much more interperatable. To check, look at the `.var_names` of you adata. If the variable names are the ensemble IDs, the gene names are often saved in a column in .var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7ca05-03fb-4575-892a-0a66b2827a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var_names\n",
    "#adata.var = adata.var.set_index('feature_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0c53a-cf7f-4c3e-a0b4-d23f45c16211",
   "metadata": {},
   "source": [
    "Before fully committing to a model, we recommend setting aside a portion of the data to test on once the model is trained. To do this, you will need to randomly split your data into train and test data, which you can do with the `train_test_split()` function. By default, this model will split the cells 70/30, but you can change the percentage you want for training using the `frac` argument. The genes are not split between the two subsets. If you intend to train your model using a lsf job, you will need to save this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c68d57-612e-4277-b0e1-83783e3a59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(adata)\n",
    "\n",
    "#save - change to your own directory\n",
    "train.write_h5ad('')\n",
    "test.write_h5ad('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be349953-7b9f-4b44-8386-391d4d8e02b5",
   "metadata": {},
   "source": [
    "This version of the train function also allows you to confirm that a list of genes is included in the genes chosen during feature selection. We tested this using the genes from Cytopus, which is a Knowledgebase of immune cells. While we found that including cytopus genes did not have a significant impact on the model performance, this tool can be useful if there are genes that you need in your final model. There are some preliminary steps to take in order to be able to use this feature: you need to make those genes into a list, then confirm that they are included in the genes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc21056-66ae-4e1f-b229-0a2e9eee1028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeBase object containing 75 cell types and 201 cellular processes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a list of all Cytopus genes \n",
    "G = cp.kb.KnowledgeBase()\n",
    "cell_dict = G.identities\n",
    "\n",
    "cp_genes = [i for i in cell_dict.values()]\n",
    "cp_genes = list(itertools.chain(*cp_genes)) #make flatlist out of LoL\n",
    "cp_genes = [x for x in cp_genes if str(x) != 'nan']\n",
    "cp_genes = np.unique(cp_genes)\n",
    "\n",
    "#confirm they these genes are included in the genes from the dataset\n",
    "cp_genes_checked = [x for x in cp_genes if x in train.var_names]\n",
    "cp_genes_checked = np.unique(cp_genes_checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad7119-3a24-49b7-bf28-9947eccf9ee0",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39313c86-3c8c-4c93-8ac4-49ace7cf21e6",
   "metadata": {},
   "source": [
    "There are many, many variables for the train_modified function. Most, but not all, are described below. To see our final recommendations for first time model training, scroll to the bottom of the list. \n",
    "1. `X`: Path to the file to be annotate or a AnnData object already loaded into memory.\n",
    "2. `labels`: Path to file with cell annotations, or an array/list in memory,  or to a column in `.obs` if X is an AnnData object\n",
    "3. `genes`: Path to file with genes for columns, or an array/list in memory, or metadata if AnnData object (aka `.var_names`)\n",
    "4. `transpose_input`: Whether to transpose X (aka if it is genexcell) \n",
    "5. `with_mean`: Whether to subtract mean values during data scaling \n",
    "6. `check_expression`: Whether to check if the matrix is normalized as expected\n",
    "9. `max_iter`: *ignore if `use_SGD = True` & `mini_batch = True`* Maximum number of iterations before reaching the minimum of the cost function. Default increases as number of cells decreases\n",
    "10. `n_jobs`: Number of CPUs used\n",
    "11. `use_SGD`: Whether or not to use SGD during training of the classifier \n",
    "12. `alpha`: *ignore if `use_SGD = False`* Strength of L2 regularization\n",
    "14. `mini_batch`: *ignore if `use_SGD = False`* Whether or not to use mini-batches during model training\n",
    "15. `batch_number`: *ignore if `use_SGD = False` & `mini_batch = False`* The number of batches per epoch\n",
    "16. `batch_size`: *ignore if `use_SGD = False` & `mini_batch = False`* The number of cells within each batch\n",
    "17. `epochs`: *ignore if `use_SGD = False` & `mini_batch = False`* The number of epochs\n",
    "18. `balance_cell_type`: *ignore if `use_SGD = False` & `mini_batch = False`* Whether to balance the cell type frequencies in mini-batches during each epoch so that rarer cell types will be sampled more\n",
    "19. `feature_selection`: Whether to perform feature selection after the first round of training and then train again using only those features\n",
    "20. `top_genes`: *ignore if `feature_selection = False`* The number of genes to select for each cell type based on their classifer coefficents \n",
    "21. `penalty`: Which regularization method to use\n",
    "22. `switch_penalty`: Whether to switch the type of regualarization for the second round of model training \n",
    "24. `use_additional`: Whether to confirm that an external list of genes are included during feature selection\n",
    "26. `additional_genes`: *ignore if `use_additional = False`* List of gene names from the external list \n",
    "27. `normalize`: *ignore if `X` is not an AnnData object* Whether or not to normalize the data in prepare_data()\n",
    "\n",
    "We recommend `train_modified(X = <X>, labels = <LABELS>, genes = <GENES>, check_expression = False, use_SGD = True, mini_batch = True, balance_cell_type = True, feature_selection = True)` which is essentially how the Teich lab trained their original CellTypist Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b11355-b7e2-4267-a0c9-e911d3341c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, genes = train_modified(X = train.X, labels = train.obs[''], genes = train.var_names, check_expression = False, use_SGD = True, mini_batch = True, balance_cell_type = True, feature_selection = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c550a5f-ae04-4826-b614-d85f1f5a2d6a",
   "metadata": {},
   "source": [
    "While it is possible to train models in your notebook, it will be much faster to train the model using an LSF job, especially for datasets larger than 100k cells. For an example python file and bsub script, see _ and _ . So that you are able to use this model again in the future, it is important to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b2bd3-38bc-4254-850e-c42b7e65b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write('') #change to your dictionary \n",
    "genes.to_csv('') #change to your dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84366274-e82b-4ac3-a134-a8e93823f402",
   "metadata": {},
   "source": [
    "### Annotating Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9790bc1-0fa9-4e1a-98e1-6a8bb25d5412",
   "metadata": {},
   "source": [
    "In order to test the quality of the model, we need to use it to annotate the test data. For an indepth explanation of using CellTypist models to annotate datasets, see 'How To Use CellTypist Models.ipynb'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb7469-cd49-456a-895d-a2f49b719f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = annotate(test, model = model)\n",
    "adata_preds = predictions.to_adata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316553d2-e579-4a48-ba4f-117debbd8d6e",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05df9d-239e-464e-9480-81620e915665",
   "metadata": {},
   "source": [
    "Before actually using this new model, it is very important to confirm that it works well. There are a few analysis we recommend doing to ensure the quality of your model. \n",
    "\n",
    "First, we recommend looking at the F1 score and Adjusted Rand Index of your new annotations compared to the groundtruth. F1 scores are the standard metric used when benchmarking cell typing models. The two will likely not match up, but from experience they typically display similar trends. When the original CellTypist model was tested on the data it was trained on, it had an F1 score of about 0.9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf559002-caa8-4c85-91c9-c4d3e2c23821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(f1_score(adata_preds.obs[''],adata_preds.obs['predicted_labels'], average=None)))\n",
    "print(adjusted_rand_score(adata_preds.obs[''], adata_preds.obs['predicted_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3c50d-2811-4503-93c4-e9c1303d8930",
   "metadata": {},
   "source": [
    "Next, we recommend looking at UMAPs coloured by the actual annotations and another by the predicted annotations to compare which cell types seem to have the most error. One thing to be aware of is that if the list of unique cell types isn't exactly the same between the two columns (eg every macrophage is labelled as a myeloid so one cell type is missing), the colour schemes of the two UMAPs will be slightly different and it will be a little harder to visually compare the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedea7a-86bb-45d1-872b-a382946cbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_preds, color = [''], title = \"Actual Labels\")\n",
    "sc.pl.umap(adata_preds, color = ['predicted_labels'], title = \"Predicted Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88d004-13e9-477e-807d-a8422976556f",
   "metadata": {},
   "source": [
    "You can also look at this data numerically by calculating the F1 score for every unique cell type in the groundtruth annotation individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f3c7f-08d2-4fec-8676-c45372a850d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list = []\n",
    "for celltype in np.unique(adata_preds.obs['']):\n",
    "    single_type = adata_preds[adata_preds.obs[''] == celltype]\n",
    "    f1 = np.median(f1_score(single_type.obs[''],single_type.obs['predicted_labels'], average=None))\n",
    "    f1_list.append([celltype, f1])\n",
    "    \n",
    "f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf53258-9eed-4175-97d3-d9acf2731448",
   "metadata": {},
   "source": [
    "You can also colour the UMAP using the confidence score, which describes how confident the classifier was in labelling that cell. This is particularly useful when considered with the UMAP coloured by the the annotations; if you have identified a cluster that seems to be largely mislabelled that also mostly has high confidence scores, that is an indication that something about the model is not working well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c4c24-d901-4607-a869-88d5a29ae123",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_preds, color = ['conf_score'], title = \"Predicted Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c0c4a-9ce2-4b7d-94af-d67554d4637d",
   "metadata": {},
   "source": [
    "If at any point concerns were raised about the model, there are a couple ways to look deeper into it to understand what is going on. The primary way would be to examine the genes that were feature selected for each cell type. There are a couple things to look at: \n",
    "1. Do these genes make sense for this cell type. This requires some knowledge about the cell types you are looking at and which genes are often associated with them. If they don't meet expectations, it may be in order to look over the original annotations to see if those are correct. \n",
    "2. Do similar cell types share a lot of genes. This may explain why cells of one cell type are often mislabelled as another related cell type. This is not as big of an issue as very dissimilar cell types getting mislabelled as each other. \n",
    "3. If you retrained the model on the same data without changing anything, are the genes chosen for each cell type largely similar. If not, this may mean that there is not a strong relationship between cell type and gene expression in your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e56ba-2f23-42cc-b873-8f3280b5341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1e6d4-6e55-413e-ba63-ef958570276d",
   "metadata": {},
   "source": [
    "This is not an exhaustive list of analyses to do to check how well your model performs. There are many more visualize your data in an informative way, such as Sankey plots and stacked bar plots, and the more indepth you look at your data, the more confident you can be in your model. These are good places to start. Once you are satisfied in your model performance, we recommend retraining the model on all of your data, without a holdout. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
